{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward SNN Trained on MNIST\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dtype=torch.float\n",
    "torch.manual_seed(734)\n",
    "print(\"Feedforward SNN Trained on MNIST\")\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=256\n",
    "data_path='./tmp/data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# # temporary dataloader if MNIST service is unavailable\n",
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ MNIST Model ##############################################################\n",
    "\n",
    "# layer parameters\n",
    "num_inputs = 28*28\n",
    "num_hidden1 = 300\n",
    "num_hidden2 = 100\n",
    "num_outputs = 10\n",
    "num_steps = 26  # for spike encoding\n",
    "beta = 0.95 #leak rate\n",
    "lr=5e-3\n",
    "#weight_decay=1e-6\n",
    "\n",
    "spike_grad1 = surrogate.atan() \n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = functional.ce_temporal_loss()\n",
    "        self.accuracy_metric = functional.accuracy_temporal\n",
    "\n",
    "        #initialise neuron connections\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(num_inputs, num_hidden1),\n",
    "            nn.Linear(num_hidden1,num_hidden2),\n",
    "            nn.Linear(num_hidden2,num_outputs)\n",
    "        ])\n",
    "\n",
    "        # initialize neurons\n",
    "        self.neurons = nn.ModuleList([\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1),\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1),\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1)\n",
    "        ])\n",
    "\n",
    "\n",
    "        #pytorch creates the tensors to represent the network layout and weights for each layer; snntorch provides the model that operates on the entire tensor (at each layer).\n",
    "\n",
    "  \n",
    "    def forward(self,x): #x is input data\n",
    "\n",
    "        #spike encoding at input layer\n",
    "        x_spk = spikegen.latency(x,num_steps=num_steps) \n",
    "        # Initialize hidden states\n",
    "        mem1 = self.neurons[0].init_leaky()\n",
    "        mem2 = self.neurons[1].init_leaky()\n",
    "        mem3 = self.neurons[2].init_leaky()\n",
    "        \n",
    "        # record spike outputs and membrane potentials\n",
    "        mem3_rec = []\n",
    "        spk3_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            x = x_spk[step] #for encoded input\n",
    "\n",
    "            cur1 = self.linears[0](x)\n",
    "            spk1, mem1 = self.neurons[0](cur1, mem1)\n",
    "\n",
    "            cur2 = self.linears[1](spk1)\n",
    "            spk2, mem2 = self.neurons[1](cur2, mem2)\n",
    "\n",
    "            cur3 = self.linears[2](spk2)\n",
    "            spk3, mem3 = self.neurons[2](cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "    \n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Minibatch stats:\n",
      "    Train Set Loss: 2.30\n",
      "    Test Set Loss: 10.73\n",
      "    Accuracy: 13.67%\n",
      "    Accuracy: 10.55%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50, Minibatch stats:\n",
      "    Train Set Loss: 2.61\n",
      "    Test Set Loss: 2.38\n",
      "    Accuracy: 13.28%\n",
      "    Accuracy: 10.55%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100, Minibatch stats:\n",
      "    Train Set Loss: 2.50\n",
      "    Test Set Loss: 2.14\n",
      "    Accuracy: 12.89%\n",
      "    Accuracy: 11.72%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150, Minibatch stats:\n",
      "    Train Set Loss: 2.11\n",
      "    Test Set Loss: 2.23\n",
      "    Accuracy: 8.98%\n",
      "    Accuracy: 7.03%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### STATS ##############\n",
    "\n",
    "def print_stats(data, targets):\n",
    "    output, _ = net(data.view(data.size(0), -1))\n",
    "    acc  = net.accuracy_metric(output, targets)\n",
    "    #dev = np.std((targets == idx).detach().cpu().numpy()) #TODO deviation for latency encoding\n",
    "    print(f\"    Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {img_counter}, Minibatch stats:\")\n",
    "    print(f\"    Train Set Loss: {loss_hist[batch_counter]:.2f}\")\n",
    "    print(f\"    Test Set Loss: {test_loss_hist[img_counter]:.2f}\")\n",
    "    print_stats(data, targets)\n",
    "    print_stats(test_data, test_targets)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "# Load the network onto CUDA\n",
    "net = Net().to(device)\n",
    "\n",
    "loss = functional.loss.ce_rate_loss()\n",
    "optimiser = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "img_counter = 0 #total no. of images iterated over\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    batch_counter=0 #image number within current batch\n",
    "\n",
    "    train_batches = iter(train_loader)\n",
    "\n",
    "    #mini-batch loop\n",
    "    for data, targets in train_batches: #torch.Size([128, 1, 28, 28]), torch.Size([128])\n",
    "\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train() #inform pytorch\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        #calculate loss as cross entropy of membrane potential at each step\n",
    "        loss_val = net.loss(spk_rec,targets)\n",
    "\n",
    "        \n",
    "        optimiser.zero_grad() #(reset for batch)\n",
    "        loss_val.backward() #calculate backpropogation error gradient\n",
    "        optimiser.step() #then update parameters\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad(): #tell pytorch to disable gradient calculation (save compute)\n",
    "            net.eval()\n",
    "\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = net.loss(test_spk,test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "            # Print train/test loss/accuracy\n",
    "            if img_counter % 50 == 0:\n",
    "                train_printer()\n",
    "            img_counter += 1\n",
    "            batch_counter +=1\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=len(mnist_test), shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    print(\"Test Set: \")\n",
    "    print_stats(data,targets)\n",
    "\n",
    "###TODO: change loss functions, latency encoding,\n",
    "# can we implement STDP?\n",
    "\n",
    "#Notes:\n",
    "#training time is almost double"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
