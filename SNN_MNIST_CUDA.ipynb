{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward SNN Trained on MNIST\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dtype=torch.float\n",
    "torch.manual_seed(734)\n",
    "print(\"Feedforward SNN Trained on MNIST\")\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device) #force cuda use\n",
    "print('Using device:', device)\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=256\n",
    "data_path='./tmp/data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# # temporary dataloader if MNIST service is unavailable\n",
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True,drop_last=True,generator=torch.Generator(device='cuda'))\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True,drop_last=True, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ MNIST Model ##############################################################\n",
    "\n",
    "# layer parameters\n",
    "num_inputs = 28*28\n",
    "num_hidden1 = 200\n",
    "num_hidden2 = 50\n",
    "num_outputs = 10\n",
    "num_steps = 26  # for spike encoding\n",
    "beta = 0.95 #leak rate\n",
    "lr=5e-3\n",
    "#weight_decay=1e-6\n",
    "\n",
    "spike_grad1 = surrogate.atan() \n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = functional.ce_count_loss()\n",
    "        self.accuracy_metric = functional.accuracy_rate\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden1)\n",
    "        self.lif1 = snn.Leaky(beta=beta,spike_grad=spike_grad1)\n",
    "\n",
    "        self.fc2 = nn.Linear(num_hidden1,num_hidden2)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad1)\n",
    "\n",
    "        self.fc3 = nn.Linear(num_hidden2,num_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad1)\n",
    "\n",
    "        #pytorch creates the tensors to represent the network layout and weights for each layer; snntorch provides the model that operates on the entire tensor (at each layer).\n",
    "\n",
    "  \n",
    "    def forward(self,x): #x is input data\n",
    "\n",
    "        #spike encoding at input layer\n",
    "        x_spk = spikegen.rate(x,num_steps=num_steps) \n",
    "        # Initialize hidden states\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif2.init_leaky()\n",
    "\n",
    "        # record spike outputs and membrane potentials\n",
    "        mem3_rec = []\n",
    "        spk3_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            x = x_spk[step] #for encoded input\n",
    "\n",
    "            cur1 = self.fc1(x)\n",
    "            \n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "    \n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Benko\\git\\UoM\\SNN\\SNN_MNIST_CUDA_tensors.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/UoM/SNN/SNN_MNIST_CUDA_tensors.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m train_batches \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/UoM/SNN/SNN_MNIST_CUDA_tensors.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m#mini-batch loop\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/UoM/SNN/SNN_MNIST_CUDA_tensors.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m train_batches: \u001b[39m#torch.Size([128, 1, 28, 28]), torch.Size([128])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/UoM/SNN/SNN_MNIST_CUDA_tensors.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/UoM/SNN/SNN_MNIST_CUDA_tensors.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 620\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:276\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m         batch \u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39m(sampler_iter) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)]\n\u001b[0;32m    277\u001b[0m         \u001b[39myield\u001b[39;00m batch\n\u001b[0;32m    278\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m         batch \u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39;49m(sampler_iter) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)]\n\u001b[0;32m    277\u001b[0m         \u001b[39myield\u001b[39;00m batch\n\u001b[0;32m    278\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:165\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n):\n\u001b[1;32m--> 165\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39;49mrandperm(n, generator\u001b[39m=\u001b[39;49mgenerator)\u001b[39m.\u001b[39;49mnumpy())\n\u001b[0;32m    166\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39mrandperm(n, generator\u001b[39m=\u001b[39mgenerator)[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m%\u001b[39m n]\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "########### STATS ##############\n",
    "\n",
    "def print_stats(data, targets):\n",
    "    output, _ = net(data.view(data.size(0), -1))\n",
    "    acc  = net.accuracy_metric(output, targets)\n",
    "    #dev = np.std((targets == idx).detach().cpu().numpy()) #TODO deviation for latency encoding\n",
    "    print(f\"    Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {img_counter}, Minibatch stats:\")\n",
    "    print(f\"    Train Set Loss: {loss_hist[batch_counter]:.2f}\")\n",
    "    print(f\"    Test Set Loss: {test_loss_hist[img_counter]:.2f}\")\n",
    "    print_stats(data, targets)\n",
    "    print_stats(test_data, test_targets)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "# Load the network onto CUDA\n",
    "net = Net().to(device)\n",
    "\n",
    "loss = functional.loss.ce_rate_loss()\n",
    "optimiser = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "img_counter = 0 #total no. of images iterated over\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    batch_counter=0 #image number within current batch\n",
    "\n",
    "    train_batches = iter(train_loader)\n",
    "\n",
    "    #mini-batch loop\n",
    "    for data, targets in train_batches: #torch.Size([128, 1, 28, 28]), torch.Size([128])\n",
    "        print(\"ok\")\n",
    "        \n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train() #inform pytorch\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        #calculate loss as cross entropy of membrane potential at each step\n",
    "        loss_val = net.loss(spk_rec,targets)\n",
    "\n",
    "        \n",
    "        optimiser.zero_grad() #(reset for batch)\n",
    "        loss_val.backward() #calculate backpropogation error gradient\n",
    "        optimiser.step() #then update parameters\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad(): #tell pytorch to disable gradient calculation (save compute)\n",
    "            net.eval()\n",
    "\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = net.loss(test_spk,test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "            # Print train/test loss/accuracy\n",
    "            if img_counter % 50 == 0:\n",
    "                train_printer()\n",
    "            img_counter += 1\n",
    "            batch_counter +=1\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=len(mnist_test), shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    print(\"Test Set: \")\n",
    "    print_stats(data,targets)\n",
    "\n",
    "###TODO: change loss functions, latency encoding,\n",
    "# can we implement STDP?\n",
    "\n",
    "#Notes:\n",
    "#training time is almost double"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
