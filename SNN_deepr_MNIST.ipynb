{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward SNN Trained on MNIST\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DTYPE=torch.float\n",
    "\n",
    "torch.manual_seed(734)\n",
    "print(\"Feedforward SNN Trained on MNIST\")\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device=torch.device(\"cpu\") #TODO\n",
    "print('Using device:', device)\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=256\n",
    "data_path='./tmp/data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# # temporary dataloader if MNIST service is unavailable\n",
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a connectivity percentage as it's easier to calculate\\\n",
    "given $\\theta_k \\leftarrow \\theta_k - \\eta \\frac{\\partial}{\\partial \\theta_k} \\mathbb{E}_{X, Y^*}(\\theta) - \\eta \\alpha + \\sqrt{2\\eta T} \\nu_k$,\\\n",
    "$-\\eta \\frac{\\partial}{\\partial \\theta_k} \\mathbb{E}_{X, Y^*}(\\theta)$ is already applied during the backward pass.\\\n",
    "Therefore in our training loop we need to apply $- \\eta \\alpha + \\sqrt{2\\eta T} \\nu_k$ to connection weights.\n",
    "\n",
    "Also we can combine the sign and mask matrix into one where we have each element either 1/-1 for the sign or 0 if inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ MNIST Model ##############################################################\n",
    "\n",
    "# layer parameters\n",
    "num_inputs = 28*28\n",
    "num_hidden1 = 200\n",
    "num_hidden2 = 50\n",
    "num_outputs = 10\n",
    "num_steps = 26  # for spike encoding\n",
    "beta = 0.95 #leak rate\n",
    "lr=5e-1\n",
    "#weight_decay=1e-6\n",
    "\n",
    "spike_grad1 = surrogate.atan()\n",
    "\n",
    "lr=1e-2 #TODO learning rate does not directly translate from the SNN model due to the iteration steps for producing spikes\n",
    "#weight_decay=1e-6\n",
    "\n",
    "class WeightMasker(nn.Module):\n",
    "\n",
    "    #TODO send tensors to CUDA\n",
    "\n",
    "    def __init__(self, linearsList : nn.ModuleList, learnRate, temp = 1e-5, alpha = 1e-5, connectivity = 1):\n",
    "        super().__init__()\n",
    "        self.learnRate = learnRate\n",
    "        self.regulariser = alpha * learnRate\n",
    "        self.noiseTemp = temp\n",
    "        self.connectivity = connectivity\n",
    "        self.weightSignMasks = [] #element of 1/-1 means connection active\n",
    "        self.linearsList = linearsList\n",
    "\n",
    "        with torch.no_grad(): #is this needed?\n",
    "        #initialise with weight connectivity %\n",
    "            for i in self.linearsList:\n",
    "                print(\"initial zero weights:\",torch.sum(i.weight==0).item())\n",
    "                weightMask = torch.bernoulli(torch.full_like(i.weight,self.connectivity,dtype=DTYPE))\n",
    "                i.weight.data = i.weight * weightMask #hadamard product\n",
    "                self.weightSignMasks.append( torch.sign(i.weight) * weightMask)\n",
    "\n",
    "\n",
    "    def deepr_update(self):\n",
    "        if not self.training: return\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.linearsList)):\n",
    "                (new_w,new_wsm) = self.rewiring(self.linearsList[i].weight,self.weightSignMasks[i])\n",
    "                self.linearsList[i].weight.data=new_w #dont use weight = nn.Parameter, or it goes haywire\n",
    "                self.weightSignMasks[i]=new_wsm\n",
    "\n",
    "    def rewiring(self, weights: torch.tensor, weightSignMask: torch.tensor):\n",
    "        #add regularisation and noise\n",
    "        #weightDiff = torch.randn_like(weights,dtype=DTYPE)*((2*self.learnRate*self.noiseTemp)**0.5) - self.regulariser\n",
    "        #weights = weights + weightDiff\n",
    "\n",
    "        #remove connections below threshold\n",
    "        remove_weights = ((weights * weightSignMask) >= 0)\n",
    "        \n",
    "        weightSignMask = weightSignMask * remove_weights\n",
    "\n",
    "        #set disabled weights to zero\n",
    "        weightMask = torch.abs(weightSignMask)\n",
    "        weights = weights * weightMask\n",
    "\n",
    "        #calculate connections to activate\n",
    "        connection_count = torch.numel(weightMask)\n",
    "        to_activate = int( (self.connectivity - (weightMask.sum()/connection_count) ) * connection_count )\n",
    "\n",
    "        if to_activate>0:\n",
    "            zero_indexes = torch.nonzero(weightMask == 0)\n",
    "\n",
    "            #randomly select disabled weights\n",
    "            zero_ind_ind = torch.randperm(zero_indexes.size(0))[:to_activate] #this produces indexes of the zero indexes list\n",
    "\n",
    "            selected_weights = zero_indexes[zero_ind_ind]\n",
    "\n",
    "            #enable weights selected with a random sign\n",
    "            new_signs = ((torch.rand(to_activate) < 0.5).float() - 0.5) * 2\n",
    "            weightSignMask[selected_weights[:,0],selected_weights[:,1]] = new_signs\n",
    "\n",
    "            #assign initial values to weights equal to the learning rate\n",
    "            weights[selected_weights[:,0],selected_weights[:,1]] = new_signs * self.learnRate\n",
    "\n",
    "        return (weights, weightSignMask)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = functional.ce_count_loss()\n",
    "        self.accuracy_metric = functional.accuracy_rate\n",
    "\n",
    "        #initialise neuron connections\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(num_inputs, num_hidden1),\n",
    "            nn.Linear(num_hidden1,num_hidden2),\n",
    "            nn.Linear(num_hidden2,num_outputs)\n",
    "        ])\n",
    "\n",
    "        self.weightMasker = WeightMasker(self.linears,lr)\n",
    "\n",
    "        # initialize neurons\n",
    "        self.neurons = nn.ModuleList([\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1),\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1),\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1)\n",
    "        ])\n",
    "\n",
    "        #pytorch creates the tensors to represent the network layout and weights for each layer; snntorch provides the model that operates on the entire tensor (at each layer).\n",
    "\n",
    "  \n",
    "    def forward(self,x): #x is input data\n",
    "\n",
    "        #spike encoding at input layer\n",
    "        x_spk = spikegen.rate(x,num_steps=num_steps) \n",
    "        # Initialize hidden states\n",
    "        \n",
    "        mem1 = self.neurons[0].init_leaky()\n",
    "        mem2 = self.neurons[1].init_leaky()\n",
    "        mem3 = self.neurons[2].init_leaky()\n",
    "        \n",
    "        mems = []\n",
    "        for n in self.neurons:\n",
    "            mems.append(n.init_leaky())\n",
    "\n",
    "        # record spike outputs and membrane potentials\n",
    "        mem3_rec = []\n",
    "        spk3_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            x = x_spk[step] #for encoded input\n",
    "\n",
    "            cur1 = self.linears[0](x)\n",
    "            spk1, mem1 = self.neurons[0](cur1, mem1)\n",
    "\n",
    "            cur2 = self.linears[1](spk1)\n",
    "            spk2, mem2 = self.neurons[1](cur2, mem2)\n",
    "\n",
    "            cur3 = self.linears[2](spk2)\n",
    "            spk3, mem3 = self.neurons[2](cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "    \n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial zero weights: 0\n",
      "initial zero weights: 0\n",
      "initial zero weights: 0\n",
      "Epoch 0, Iteration 0, Minibatch stats:\n",
      "    Train Set Loss: 2.79\n",
      "    Test Set Loss: 5.69\n",
      "    Accuracy: 9.38%\n",
      "    Accuracy: 10.55%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50, Minibatch stats:\n",
      "    Train Set Loss: 2.31\n",
      "    Test Set Loss: 2.30\n",
      "    Accuracy: 6.25%\n",
      "    Accuracy: 7.81%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100, Minibatch stats:\n",
      "    Train Set Loss: 2.30\n",
      "    Test Set Loss: 2.30\n",
      "    Accuracy: 10.94%\n",
      "    Accuracy: 9.77%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150, Minibatch stats:\n",
      "    Train Set Loss: 2.30\n",
      "    Test Set Loss: 2.30\n",
      "    Accuracy: 8.20%\n",
      "    Accuracy: 10.16%\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Benko\\git\\benkol003\\SNN\\SNN_deepr_MNIST.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/benkol003/SNN/SNN_deepr_MNIST.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m loss_val \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mloss(spk_rec,targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/benkol003/SNN/SNN_deepr_MNIST.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad() \u001b[39m#(reset for batch)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/benkol003/SNN/SNN_deepr_MNIST.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m loss_val\u001b[39m.\u001b[39;49mbackward() \u001b[39m#calculate backpropogation error gradient\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/benkol003/SNN/SNN_deepr_MNIST.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m optimiser\u001b[39m.\u001b[39mstep() \u001b[39m#then update parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benko/git/benkol003/SNN/SNN_deepr_MNIST.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m net\u001b[39m.\u001b[39mweightMasker\u001b[39m.\u001b[39mdeepr_update() \u001b[39m#then apply deepr\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    287\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 288\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\snntorch\\surrogate.py:197\u001b[0m, in \u001b[0;36mATan.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m    194\u001b[0m (input_,) \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39msaved_tensors\n\u001b[0;32m    195\u001b[0m grad_input \u001b[39m=\u001b[39m grad_output\u001b[39m.\u001b[39mclone()\n\u001b[0;32m    196\u001b[0m grad \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 197\u001b[0m     ctx\u001b[39m.\u001b[39;49malpha\n\u001b[0;32m    198\u001b[0m     \u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[0;32m    199\u001b[0m     \u001b[39m/\u001b[39;49m (\u001b[39m1\u001b[39;49m \u001b[39m+\u001b[39;49m (math\u001b[39m.\u001b[39;49mpi \u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m ctx\u001b[39m.\u001b[39;49malpha \u001b[39m*\u001b[39;49m input_)\u001b[39m.\u001b[39;49mpow_(\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    200\u001b[0m     \u001b[39m*\u001b[39m grad_input\n\u001b[0;32m    201\u001b[0m )\n\u001b[0;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m grad, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:913\u001b[0m, in \u001b[0;36mTensor.__rdiv__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rdiv__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 913\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreciprocal() \u001b[39m*\u001b[39m other\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########### STATS ##############\n",
    "\n",
    "def print_stats(data, targets):\n",
    "    output, _ = net(data.view(data.size(0), -1))\n",
    "    acc  = net.accuracy_metric(output, targets)\n",
    "    #dev = np.std((targets == idx).detach().cpu().numpy()) #TODO deviation for latency encoding\n",
    "    print(f\"    Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {img_counter}, Minibatch stats:\")\n",
    "    print(f\"    Train Set Loss: {loss_hist[batch_counter]:.2f}\")\n",
    "    print(f\"    Test Set Loss: {test_loss_hist[img_counter]:.2f}\")\n",
    "    print_stats(data, targets)\n",
    "    print_stats(test_data, test_targets)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "# Load the network onto CUDA\n",
    "net = Net().to(device)\n",
    "\n",
    "loss = functional.loss.ce_rate_loss()\n",
    "optimiser = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimiser, mode='min', factor=0.2, patience=5)\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "img_counter = 0 #total no. of images iterated over\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    batch_counter=0 #image number within current batch\n",
    "\n",
    "    train_batches = iter(train_loader)\n",
    "\n",
    "    #mini-batch loop\n",
    "    for data, targets in train_batches: #torch.Size([128, 1, 28, 28]), torch.Size([128])\n",
    "\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train() #inform pytorch\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        #calculate loss as cross entropy of membrane potential at each step\n",
    "        loss_val = net.loss(spk_rec,targets)\n",
    "\n",
    "        \n",
    "        optimiser.zero_grad() #(reset for batch)\n",
    "        loss_val.backward() #calculate backpropogation error gradient\n",
    "        optimiser.step() #then update parameters\n",
    "\n",
    "        net.weightMasker.deepr_update() #then apply deepr\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad(): #tell pytorch to disable gradient calculation (save compute)\n",
    "            net.eval()\n",
    "\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = net.loss(test_spk,test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "            # Print train/test loss/accuracy\n",
    "            if img_counter % 50 == 0:\n",
    "                train_printer()\n",
    "            img_counter += 1\n",
    "            batch_counter +=1\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=len(mnist_test), shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    print(\"Test Set: \")\n",
    "    print_stats(data,targets)\n",
    "\n",
    "###TODO: change loss functions, latency encoding,\n",
    "# can we implement STDP?\n",
    "\n",
    "#Notes:\n",
    "#training time is almost double"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
