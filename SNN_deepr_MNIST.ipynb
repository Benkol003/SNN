{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward SNN Trained on MNIST\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dtype=torch.float\n",
    "torch.manual_seed(734)\n",
    "print(\"Feedforward SNN Trained on MNIST\")\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device=torch.device(\"cpu\") #TODO\n",
    "print('Using device:', device)\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=256\n",
    "data_path='./tmp/data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# # temporary dataloader if MNIST service is unavailable\n",
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a connectivity percentage as it's easier to calculate\\\n",
    "given $\\theta_k \\leftarrow \\theta_k - \\eta \\frac{\\partial}{\\partial \\theta_k} \\mathbb{E}_{X, Y^*}(\\theta) - \\eta \\alpha + \\sqrt{2\\eta T} \\nu_k$,\\\n",
    "$-\\eta \\frac{\\partial}{\\partial \\theta_k} \\mathbb{E}_{X, Y^*}(\\theta)$ is already applied during the backward pass.\\\n",
    "Therefore in our training loop we need to apply $- \\eta \\alpha + \\sqrt{2\\eta T} \\nu_k$ to connection weights.\n",
    "\n",
    "Also we can combine the sign and mask matrix into one where we have each element either 1/-1 for the sign or 0 if inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ MNIST Model ##############################################################\n",
    "\n",
    "# layer parameters\n",
    "num_inputs = 28*28\n",
    "num_hidden1 = 200\n",
    "num_hidden2 = 50\n",
    "num_outputs = 10\n",
    "num_steps = 26  # for spike encoding\n",
    "beta = 0.95 #leak rate\n",
    "lr=5e-3\n",
    "#weight_decay=1e-6\n",
    "\n",
    "spike_grad1 = surrogate.atan()\n",
    "\n",
    "class WeightMasker(nn.Module):\n",
    "\n",
    "    #TODO try sending tensors to CUDA\n",
    "\n",
    "    def __init__(self, linearsList : nn.ModuleList, learnRate, temp = 0.1, alpha = 0.0, connectivity = 1):\n",
    "        super().__init__()\n",
    "        self.learnRate = learnRate\n",
    "        self.regulariser = alpha * learnRate\n",
    "        self.noiseTemp = temp\n",
    "        self.connectivity = connectivity\n",
    "        self.weightSignMasks = [] #element of 1/-1 means connection active\n",
    "        self.linearsList = linearsList #TODO this may not work as a reference\n",
    "\n",
    "        with torch.no_grad(): #is this needed?\n",
    "        #initialise with weight connectivity %\n",
    "            for i in self.linearsList:\n",
    "                weightMask = torch.bernoulli(torch.full_like(i.weight,self.connectivity))\n",
    "                i.weight = nn.Parameter(i.weight * weightMask) #hadamard product\n",
    "                #TODO difference bteween this and assigning to i.weight.data\n",
    "                self.weightSignMasks.append( torch.sign(i.weight) * weightMask)#TODO is masking i.weight within sign faster? /does it work? (i.weight[weightMask])\n",
    "\n",
    "    def deepr_update(self):\n",
    "        if not self.training: return\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.linearsList)):\n",
    "                #update weights with rest of terms in update rule\n",
    "                self.weightDiff = torch.randn_like(self.linearsList[i].weight)*((2*self.learnRate*self.noiseTemp)**0.5) - self.regulariser\n",
    "                self.linearsList[i].weight = nn.Parameter(self.linearsList[i].weight + self.weightDiff)\n",
    "\n",
    "                #remove connections below threshold\n",
    "                remove_weights = (self.linearsList[i].weight * self.weightSignMasks[i]) >= 0 #set to 0 ones to discard\n",
    "\n",
    "                #update mask\n",
    "                self.weightSignMasks[i] = self.weightSignMasks[i] * remove_weights #would seperate sign and mask matricies be faster using masks and bitwise & instead?\n",
    "\n",
    "                #set disabled weights to zero\n",
    "                weightMask = torch.abs(self.weightSignMasks[i])\n",
    "                self.linearsList[i].weight = nn.Parameter(self.linearsList[i].weight * weightMask)\n",
    "\n",
    "                ### Ensure there are enough active connections\n",
    "                #TODO atm this isnt true to the paper, the connectivity is calculated GLOBALLY. i.e. some layers may be more sparse than others.\n",
    "                #this is just easier to implement atm for testing\n",
    "                i_numel = torch.numel(weightMask)\n",
    "                to_activate = int( (self.connectivity - (weightMask.sum() / i_numel)) * i_numel )\n",
    "                #print(\"connectivity: \",(weightMask.sum() / i_numel)) #tf is going on here, it hovers around 50%\n",
    "                #print(\"to activate: \",to_activate)\n",
    "                if to_activate>0:\n",
    "                    # Get the indices of the elements that are equal to zero\n",
    "                    zero_indicies = torch.nonzero(self.weightSignMasks[i]==0) #why does torch.where return a tuple?\n",
    "\n",
    "                    # Randomly select n indices\n",
    "                    zero_ind_ind = torch.randperm(zero_indicies.size(0))[:to_activate] #TODO inefficient\n",
    "                    # see https://stackoverflow.com/questions/59461811/random-choice-with-pytorch\n",
    "\n",
    "                    # Get the selected indices\n",
    "                    selected_weights = zero_indicies[zero_ind_ind] #weight indicies\n",
    "\n",
    "                    #enable the weights selected\n",
    "                    self.weightSignMasks[i][selected_weights[:,0],selected_weights[:,1]] = 1 #TODO RM should already be =0\n",
    "\n",
    "                    #assign a random sign\n",
    "                    self.weightSignMasks[i][selected_weights[:,0],selected_weights[:,1]] = ((torch.rand(to_activate) < 0.5).float() - 0.5) * 2\n",
    "\n",
    "                    #enabled weights should already be initialised to zero\n",
    "        \n",
    "\n",
    "    def forward_pre_hook(self,module,args):\n",
    "        pass\n",
    "\n",
    "    def back_hook(self,module,grad_input,grad_output):\n",
    "        pass #TODO only allow gradient updates of enabled connections (move out of deepr_update)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = functional.ce_count_loss()\n",
    "        self.accuracy_metric = functional.accuracy_rate\n",
    "\n",
    "        #initialise neuron connections\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(num_inputs, num_hidden1),\n",
    "            nn.Linear(num_hidden1,num_hidden2),\n",
    "            nn.Linear(num_hidden2,num_outputs)\n",
    "        ])\n",
    "\n",
    "        self.weightMasker = WeightMasker(self.linears,lr)\n",
    "\n",
    "        # initialize neurons\n",
    "        self.neurons = nn.ModuleList([\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1),\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1),\n",
    "            snn.Leaky(beta=beta,spike_grad=spike_grad1)\n",
    "        ])\n",
    "\n",
    "        #pytorch creates the tensors to represent the network layout and weights for each layer; snntorch provides the model that operates on the entire tensor (at each layer).\n",
    "\n",
    "  \n",
    "    def forward(self,x): #x is input data\n",
    "\n",
    "        #spike encoding at input layer\n",
    "        x_spk = spikegen.rate(x,num_steps=num_steps) \n",
    "        # Initialize hidden states\n",
    "        \n",
    "        mem1 = self.neurons[0].init_leaky()\n",
    "        mem2 = self.neurons[1].init_leaky()\n",
    "        mem3 = self.neurons[2].init_leaky()\n",
    "        \n",
    "        mems = []\n",
    "        for n in self.neurons:\n",
    "            mems.append(n.init_leaky())\n",
    "\n",
    "        # record spike outputs and membrane potentials\n",
    "        mem3_rec = []\n",
    "        spk3_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            x = x_spk[step] #for encoded input\n",
    "\n",
    "            cur1 = self.linears[0](x)\n",
    "            spk1, mem1 = self.neurons[0](cur1, mem1)\n",
    "\n",
    "            cur2 = self.linears[1](spk1)\n",
    "            spk2, mem2 = self.neurons[1](cur2, mem2)\n",
    "\n",
    "            cur3 = self.linears[2](spk2)\n",
    "            spk3, mem3 = self.neurons[2](cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "    \n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Minibatch stats:\n",
      "    Train Set Loss: 2.79\n",
      "    Test Set Loss: 3.17\n",
      "    Accuracy: 8.59%\n",
      "    Accuracy: 8.98%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50, Minibatch stats:\n",
      "    Train Set Loss: 14.53\n",
      "    Test Set Loss: 14.39\n",
      "    Accuracy: 5.47%\n",
      "    Accuracy: 3.12%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########### STATS ##############\n",
    "\n",
    "def print_stats(data, targets):\n",
    "    output, _ = net(data.view(data.size(0), -1))\n",
    "    acc  = net.accuracy_metric(output, targets)\n",
    "    #dev = np.std((targets == idx).detach().cpu().numpy()) #TODO deviation for latency encoding\n",
    "    print(f\"    Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {img_counter}, Minibatch stats:\")\n",
    "    print(f\"    Train Set Loss: {loss_hist[batch_counter]:.2f}\")\n",
    "    print(f\"    Test Set Loss: {test_loss_hist[img_counter]:.2f}\")\n",
    "    print_stats(data, targets)\n",
    "    print_stats(test_data, test_targets)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "# Load the network onto CUDA\n",
    "net = Net().to(device)\n",
    "\n",
    "loss = functional.loss.ce_rate_loss()\n",
    "optimiser = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "img_counter = 0 #total no. of images iterated over\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    batch_counter=0 #image number within current batch\n",
    "\n",
    "    train_batches = iter(train_loader)\n",
    "\n",
    "    #mini-batch loop\n",
    "    for data, targets in train_batches: #torch.Size([128, 1, 28, 28]), torch.Size([128])\n",
    "\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train() #inform pytorch\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        #calculate loss as cross entropy of membrane potential at each step\n",
    "        loss_val = net.loss(spk_rec,targets)\n",
    "\n",
    "        \n",
    "        optimiser.zero_grad() #(reset for batch)\n",
    "        loss_val.backward() #calculate backpropogation error gradient\n",
    "        optimiser.step() #then update parameters\n",
    "\n",
    "        net.weightMasker.deepr_update() #then apply deepr\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad(): #tell pytorch to disable gradient calculation (save compute)\n",
    "            net.eval()\n",
    "\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = net.loss(test_spk,test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "            # Print train/test loss/accuracy\n",
    "            if img_counter % 50 == 0:\n",
    "                train_printer()\n",
    "            img_counter += 1\n",
    "            batch_counter +=1\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=len(mnist_test), shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    print(\"Test Set: \")\n",
    "    print_stats(data,targets)\n",
    "\n",
    "###TODO: change loss functions, latency encoding,\n",
    "# can we implement STDP?\n",
    "\n",
    "#Notes:\n",
    "#training time is almost double"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
