{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stochastic neurons for DVS temporal cascade layer\n",
    "### ------------------ LOOKS LIKE I WONT BE USING THIS (YET) --------------------\n",
    "\n",
    "### experimentation for an input event/ spike to produces output spikes that decrease in rate since the time of the input event\n",
    "\n",
    "### linear leak, stochastic spike generation\n",
    "##assume we collect and send events in 16ms bursts\n",
    "\n",
    "class StochasticLinearDecay():\n",
    "    #this class implements a temporal cascade layer; events are accumulated and decay; output events are stochastically generated related to the stored decay value\n",
    "    def __init__(self, size):\n",
    "        self.SIZE = size\n",
    "        #self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = torch.zeros(self.SIZE)\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        #TODO when is the accumulated buffer releasec?\n",
    "        \n",
    "        #decay state\n",
    "        self.state = torch.clamp(self.state - (1/32),0,1) #decay window may be 32 ms but buffers cache events over 16ms (which have to do elsewhere)\n",
    "\n",
    "        #update state with input events\n",
    "        self.state = torch.clamp(input+self.state,0,1)\n",
    "\n",
    "        #stochastically generate output spikes/events\n",
    "        events = torch.bernoulli(self.state)\n",
    "\n",
    "        return events\n",
    "\n",
    "\n",
    "event_rec = []\n",
    "state_rec = []\n",
    "cascade_layer = StochasticLinearDecay((1))\n",
    "cascade_layer.forward(torch.ones((1)))\n",
    "event_rec.append(events)\n",
    "state_rec.append(cascade_layer.state)\n",
    "for i in range(64):\n",
    "    events= cascade_layer.forward(torch.tensor(0.0))\n",
    "    event_rec.append(events)\n",
    "    state_rec.append(cascade_layer.state)\n",
    "\n",
    "plt.plot(state_rec)\n",
    "plt.show()\n",
    "plt.plot(event_rec)\n",
    "plt.show()\n",
    "\n",
    "for i in event_rec:\n",
    "    print(i.detach().numpy(),end=\",\")\n",
    "### TODO use custom neuron instead\n",
    "### custom neuron model needed for the input layer\n",
    "'''\n",
    "class StochasticLinearDecayNeuron():\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,input_):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def detach_hidden(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def reset_hidden(cls):\n",
    "'''\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "### Ok, lets try the full temporal filter\n",
    "\n",
    "# i think the first layer doesnt perform any caching?\n",
    "\n",
    "class TemporalFilter():\n",
    "    def __init__(self):\n",
    "        self.buffer_timer = 0\n",
    "        self.filters = [torch.zeros((64,64))] + [StochasticLinearDecay((64,64))] * 5\n",
    "\n",
    "    def forward(input):\n",
    "        if self.buffer_timer % 16 == 0:\n",
    "            #then cascade \n",
    "\n",
    "\n",
    "        self.buffer_timer += 1\n",
    "\n",
    "temporal_filter = TemporalFilter()\n",
    "\n",
    "temporal_filter.filters\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
