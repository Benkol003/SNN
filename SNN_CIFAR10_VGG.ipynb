{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as layer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from trainer import *\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, functional, BatchNormTT2d\n",
    "\n",
    "from line_profiler import LineProfiler, profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-block CNN SNN Trained on cifar10\n",
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train batches: 196\n",
      "Test batches: 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dtype=torch.float\n",
    "print(\"VGG-block CNN SNN Trained on cifar10\")\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=256\n",
    "data_path='./tmp/data/cifar10/'\n",
    "num_classes = 10  # cifar has 10 output classes\n",
    "\n",
    "# Define a transform\n",
    "transform1 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,)),\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomResizedCrop(size=(32, 32),scale=(0.8,1), antialias=True),\n",
    "            ])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,)),\n",
    "            ])\n",
    "\n",
    "cifar_train = datasets.CIFAR10(data_path, train=True, download=True,transform=transform1)\n",
    "cifar_test = datasets.CIFAR10(data_path, train=False, download=True,transform=transform2)\n",
    "\n",
    "train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=True,drop_last=False,pin_memory=True,num_workers=1)\n",
    "test_loader = DataLoader(cifar_test, batch_size=batch_size, shuffle=True,drop_last=False,pin_memory=True,num_workers=1)\n",
    "\n",
    "print(\"Train batches:\",len(train_loader))\n",
    "print(\"Test batches:\",len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful to know what a convolutional layers output dimension is given by $n_{out}=\\frac{n_{in}+2p-k}{s}+1$, \n",
    "with default padding=0 stride=1 $n_{out}=n_{in}-k+1$.\n",
    "For max pool with defaults, $\\lfloor\\frac{n}{2}\\rfloor$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################ DVS Gesture Model #############################\n",
    "\n",
    "# layer parameters\n",
    "\n",
    "lr=1e-4\n",
    "\n",
    "spike_grad1 = surrogate.atan()\n",
    "\n",
    "num_steps = 11\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = functional.ce_count_loss()\n",
    "        self.accuracy_metric = functional.accuracy_rate\n",
    "\n",
    "        #initialise neuron connections\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3,32,3,padding=1),\n",
    "            BatchNormTT2d(32,num_steps),\n",
    "            nn.Conv2d(32,32,3,padding=1),\n",
    "            BatchNormTT2d(32,num_steps),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            BatchNormTT2d(64,num_steps),\n",
    "            nn.Conv2d(64,64,3,padding=1),\n",
    "            BatchNormTT2d(64,num_steps),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Linear(4096,256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256,11),      \n",
    "        ])\n",
    "\n",
    "        # initialize neurons\n",
    "        self.neurons = nn.ModuleList(\n",
    "            [snn.Leaky(beta=0.95,threshold=1,spike_grad=spike_grad1)] * len(self.layers)\n",
    "        )\n",
    "\n",
    "        self.to(device) #yes, this is needed twice\n",
    "\n",
    "        #pytorch creates the tensors to represent the network layout and weights for each layer; snntorch provides the model that operates on the entire tensor (at each layer).\n",
    "\n",
    "    def forward(self,x): #x is input data\n",
    "        #events should be treated as spikes i.e. already encoded\n",
    "\n",
    "        # Initialize hidden states\n",
    "        mem = []\n",
    "        for i in range(len(self.layers)):\n",
    "            mem.append(self.neurons[i].init_leaky())\n",
    "        \n",
    "        # record spike outputs\n",
    "        spk_rec = []\n",
    "\n",
    "        x = x.unsqueeze(0).repeat(num_steps, 1, 1, 1, 1)\n",
    "        #x_spk = spikegen.rate(x,num_steps=num_steps) \n",
    "\n",
    "        for step in range(num_steps):\n",
    "            #form inputs\n",
    "            spk_i = x[step]\n",
    "\n",
    "\n",
    "            for i in range(len(self.layers)):\n",
    "\n",
    "                if(i==18):\n",
    "                    spk_i = self.layers[i](spk_i)\n",
    "                    continue\n",
    "                elif i in {1,3,6,8}:\n",
    "                    spk_i = self.layers[i][step](spk_i)\n",
    "                    continue\n",
    "\n",
    "                if(i==10): #need to flatten from pooling to Linear\n",
    "                    spk_i = torch.flatten(spk_i,start_dim=1)\n",
    "\n",
    "                cur_i = self.layers[i](spk_i)\n",
    "                spk_i, mem[i] = self.neurons[i](cur_i,mem[i])\n",
    "                        \n",
    "\n",
    "            spk_rec.append(spk_i)\n",
    "            \n",
    "\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0)\n",
    "    \n",
    "###################################################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnet = Net().to(device)\\ndef p():\\n    optimiser = torch.optim.Adam(net.parameters(),lr=lr,weight_decay=0.001)\\n    net.train()\\n    d,t = next(iter(train_loader))\\n    d = d.to(device)\\n    t = t.to(device)\\n    logits = net(d)\\n    optimiser.zero_grad()\\n    loss = net.loss(logits,t)\\n    loss.backward()\\n    optimiser.step()\\n\\n\\nprofiler = LineProfiler()\\nprofiler.add_function(p)\\nprofiler.add_function(net.forward)\\n\\nprofiler.run('p()')\\nprofiler.print_stats()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "net = Net().to(device)\n",
    "def p():\n",
    "    optimiser = torch.optim.Adam(net.parameters(),lr=lr,weight_decay=0.001)\n",
    "    net.train()\n",
    "    d,t = next(iter(train_loader))\n",
    "    d = d.to(device)\n",
    "    t = t.to(device)\n",
    "    logits = net(d)\n",
    "    optimiser.zero_grad()\n",
    "    loss = net.loss(logits,t)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(p)\n",
    "profiler.add_function(net.forward)\n",
    "\n",
    "profiler.run('p()')\n",
    "profiler.print_stats()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd9e4c4a5cf4e8ab7afb0cf516fd5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training progress::   0%|          | 0/1960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Training loss: 2.41\n",
      "Validation loss: 2.40\n",
      "Validation accuracy: 8.59%\n",
      "Training accuracy: 11.72%\n",
      "----------------\n",
      "Iteration: 250\n",
      "Training loss: 1.83\n",
      "Validation loss: 1.67\n",
      "Validation accuracy: 38.28%\n",
      "Training accuracy: 32.81%\n",
      "----------------\n",
      "Iteration: 500\n",
      "Training loss: 1.54\n",
      "Validation loss: 1.44\n",
      "Validation accuracy: 45.70%\n",
      "Training accuracy: 41.02%\n",
      "----------------\n",
      "Iteration: 750\n",
      "Training loss: 1.49\n",
      "Validation loss: 1.39\n",
      "Validation accuracy: 48.44%\n",
      "Training accuracy: 44.14%\n",
      "----------------\n",
      "Iteration: 1000\n",
      "Training loss: 1.47\n",
      "Validation loss: 1.44\n",
      "Validation accuracy: 47.27%\n",
      "Training accuracy: 41.41%\n",
      "----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr,weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m gen_reset() \u001b[38;5;66;03m#reset the PRNG generators for the random samplers so we consistently get the same sequence of samples for each experiment run\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mvalid_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalid_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdeepr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m gen_reset()\n\u001b[1;32m      7\u001b[0m a \u001b[38;5;241m=\u001b[39m test_stats(net,test_loader\u001b[38;5;241m=\u001b[39mtest_loader,iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/mnt/windows/Users/Benko/git/benkol003/SNN/trainer.py:122\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(net, train_loader, valid_loader, optimiser, scheduler, epochs, iterations, valid_after, valid_iterations, deepr, model_path, device)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 122\u001b[0m loss_v \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#store train loss history\u001b[39;00m\n\u001b[1;32m    124\u001b[0m loss_hist\u001b[38;5;241m.\u001b[39mappend(loss_v) \u001b[38;5;66;03m#maybe use dict instead to avoid axis scaling\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = \"./models/SNN_VGG_CIFAR10.pt\"\n",
    "net = Net()\n",
    "optimiser = torch.optim.Adam(net.parameters(),lr=lr,weight_decay=0)\n",
    "gen_reset() #reset the PRNG generators for the random samplers so we consistently get the same sequence of samples for each experiment run\n",
    "net = trainer(net,train_loader=train_loader,valid_loader=test_loader,model_path=model_path,optimiser=optimiser,epochs=10,iterations=None,valid_after=250,valid_iterations=1,deepr=False,device=device)\n",
    "gen_reset()\n",
    "a = test_stats(net,test_loader=test_loader,iterations=None,device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
