{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from  torch.utils.data import random_split\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional\n",
    "\n",
    "import tonic\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward SNN Trained on Spiking Hiedelberg Digits\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "downsample_size = (128,128) #i think that downsampling removes too much information \n",
    "slice_meta_path = \"./tmp/SHD_sliced/\"\n",
    "\n",
    "dtype=torch.float\n",
    "\n",
    "print(\"Feedforward SNN Trained on Spiking Hiedelberg Digits\")\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=32 #you wanna set this so that the gpu uses all its dedicated memory but not any shared i.e. not swapping to main ram\n",
    "data_path='./tmp/data/SHD/'\n",
    "num_classes = 10  # 10 digits\n",
    "\n",
    "frame_time = 60\n",
    "\n",
    "num_steps = 10  # assuming 16 steps as we use 16ms filter window, chip runs at 1ms; can also get decent accuracy using 16 timesteps on spiking MNIST\n",
    "# 60 frames 1ms or 5ms apart\n",
    "\n",
    "transform1 = tonic.transforms.Compose([tonic.transforms.CropTime(max=frame_time*num_steps*1000),tonic.transforms.To(sensor_size=(128,128,2),n_time_bins=num_steps), lambda x: x.astype(np.float32)]) #for some reason using time_window results in sometimes getting 95 frames\n",
    "\n",
    "#downsample to stay consistent with paper + helps save memory\n",
    "\n",
    "#train_cifardvs = tonic.datasets.CIFAR10DVS(data_path)\n",
    "shd = tonic.datasets.SHD(data_path,transform=transform1)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(shd))\n",
    "test_size = len(shd) - train_size\n",
    "\n",
    "#80% train, 20% test split\n",
    "train_size = int(0.8 * len(shd))\n",
    "test_size = len(shd) - train_size\n",
    "train_shd, test_shd = random_split(shd, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_shd, batch_size=batch_size, shuffle=True,drop_last=False,collate_fn=tonic.collation.PadTensors(batch_first=True))\n",
    "test_loader = DataLoader(test_shd, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples:  6524\n",
      "testing samples:  1632\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 491 is out of bounds for axis 2 with size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining samples: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(train_shd))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting samples: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(test_shd))\n\u001b[1;32m----> 4\u001b[0m data, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m data\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\tonic\\datasets\\hsd.py:31\u001b[0m, in \u001b[0;36mHSD.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     29\u001b[0m target \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][index]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\tonic\\transforms.py:32\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, events)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m events\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\tonic\\transforms.py:894\u001b[0m, in \u001b[0;36mToFrame.__call__\u001b[1;34m(self, events)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, events):\n\u001b[1;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame_numpy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43msensor_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensor_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_time_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_time_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_event_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_event_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_incomplete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_incomplete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python310\\lib\\site-packages\\tonic\\functional\\to_frame.py:94\u001b[0m, in \u001b[0;36mto_frame_numpy\u001b[1;34m(events, sensor_size, time_window, event_count, n_time_bins, n_event_bins, overlap, include_incomplete)\u001b[0m\n\u001b[0;32m     90\u001b[0m     frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m     91\u001b[0m         (\u001b[38;5;28mlen\u001b[39m(event_slices), sensor_size[\u001b[38;5;241m2\u001b[39m], sensor_size[\u001b[38;5;241m0\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, event_slice \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(event_slices):\n\u001b[1;32m---> 94\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_slice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_slice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frames\n",
      "\u001b[1;31mIndexError\u001b[0m: index 491 is out of bounds for axis 2 with size 128"
     ]
    }
   ],
   "source": [
    "print(\"training samples: \",len(train_shd))\n",
    "print(\"testing samples: \",len(test_shd))\n",
    "\n",
    "data, targets = next(iter(train_loader))\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################ DVS Gesture Model #############################\n",
    "\n",
    "# (without temporal layer - this is up for interpretation)\n",
    "\n",
    "# layer parameters\n",
    "\n",
    "beta = 0.95\n",
    "lr=1e-4\n",
    "#weight_decay=1e-6\n",
    "\n",
    "spike_grad1 = surrogate.atan()\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = functional.ce_count_loss()\n",
    "        self.accuracy_metric = functional.accuracy_rate\n",
    "\n",
    "        #initialise neuron connections\n",
    "        #from table 1 in DVS paper\n",
    "        #TODO im multiplying features by two as paper states 6 input features, but we have seperate channels for increase/decrease events\n",
    "        self.layers = nn.ModuleList([\n",
    "            #64x64\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(2,64,3),\n",
    "            nn.Conv2d(64,128,3),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(128,128,3),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Linear(4608,256),\n",
    "            nn.Linear(256,11)\n",
    "        ])\n",
    "\n",
    "        # initialize neurons\n",
    "        self.neurons = nn.ModuleList(\n",
    "            [snn.Leaky(beta=beta,spike_grad=spike_grad1)] * 8\n",
    "        )\n",
    "\n",
    "        self.to(device) #yes, this is needed twice\n",
    "        \n",
    "        #pytorch creates the tensors to represent the network layout and weights for each layer; snntorch provides the model that operates on the entire tensor (at each layer).\n",
    "\n",
    "  \n",
    "    def forward(self,x): #x is input data\n",
    "        #events should be treated as spikes i.e. already encoded\n",
    "\n",
    "        # Initialize hidden states\n",
    "        mem = []\n",
    "        for i in range(8):\n",
    "            mem.append(self.neurons[i].init_leaky())\n",
    "        \n",
    "        # record spike outputs\n",
    "        spk_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            #form inputs\n",
    "            spk_i = x[:,step]\n",
    "\n",
    "            for i in range(8):\n",
    "                if(i==6): #need to flatten from AvgPool to Linear\n",
    "                    spk_i = torch.flatten(spk_i,start_dim=1)\n",
    "                cur_i = self.layers[i](spk_i)\n",
    "                spk_i, mem[i] = self.neurons[i](cur_i,mem[i])\n",
    "\n",
    "            spk_rec.append(spk_i)\n",
    "            \n",
    "\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0) #TODO do i really need to return membrane potentials\n",
    "    \n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''a = next(iter(test_loader))\n",
    "print(a[0].shape)\n",
    "print(a[0].dtype)\n",
    "\n",
    "net = Net().to(device)\n",
    "net.train()\n",
    "r = net.forward(a[0].to(device))\n",
    "print(\"r: \",r.shape)\n",
    "print(torch.max(r))'''\n",
    "#comment this out to reduce GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "########### STATS ##############\n",
    "\n",
    "def print_stats(data, targets):\n",
    "    output = net(data)\n",
    "    acc  = net.accuracy_metric(output, targets)\n",
    "    #dev = np.std((targets == idx).detach().cpu().numpy()) #TODO deviation for latency encoding\n",
    "    print(f\"    Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {img_counter}, Minibatch stats:\")\n",
    "    print(f\"    Train Set Loss: {loss_hist[batch_counter]:.2f}\")\n",
    "    print(f\"    Test Set Loss: {test_loss_hist[img_counter]:.2f}\")\n",
    "    print_stats(test_data, test_targets)\n",
    "    print(\"Train batch: \")\n",
    "    print_stats(data, targets)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "############################## MAIN TRAINING LOOP ###########################################\n",
    "\n",
    "\n",
    "# Load the network onto CUDA\n",
    "net = Net().to(device)\n",
    "optimiser = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #mini-batch loop\n",
    "    for data, targets in tqdm(iter(train_loader)):\n",
    "\n",
    "        data = data.float().to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        net.train() #inform pytorch\n",
    "        spk_rec = net(data)\n",
    "\n",
    "        #calculate loss as cross entropy of membrane potential at each step\n",
    "        loss_val = net.loss(spk_rec,targets)\n",
    "\n",
    "        \n",
    "        optimiser.zero_grad() #(reset for batch)\n",
    "        loss_val.backward() #calculate backpropogation error gradient\n",
    "        optimiser.step() #then update parameters\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        iterations +=1\n",
    "\n",
    "        # Test set\n",
    "        if iterations % 50 == 0:\n",
    "                \n",
    "            with torch.no_grad(): #tell pytorch to disable gradient calculation (save compute)\n",
    "                net.eval()\n",
    "\n",
    "                test_data, test_targets = next(iter(test_loader))\n",
    "                test_data = test_data.to(device)\n",
    "                test_targets = test_targets.to(device)\n",
    "\n",
    "                # Test set forward pass\n",
    "                test_spk = net(test_data)\n",
    "\n",
    "                # Test set loss\n",
    "                test_loss = net.loss(test_spk,test_targets)\n",
    "                test_loss_hist.append(test_loss.item())\n",
    "                # Print train/test loss/accuracy\n",
    "                train_printer()\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(),\"./models/DVS2_lr_1e-4.pt\")\n",
    "'''\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(\"./models/DVS2_lr_1e-4.pt\"))\n",
    "#loss graph wont be after reload TODO\n",
    "'''\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  accs = []\n",
    "  for i, (data, targets) in enumerate(test_loader):\n",
    "    if i>100: break\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    output = net.forward(data)\n",
    "    accs.append(net.accuracy_metric(output,targets))\n",
    "  \n",
    "  print(\"mean accuracy: \",np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
